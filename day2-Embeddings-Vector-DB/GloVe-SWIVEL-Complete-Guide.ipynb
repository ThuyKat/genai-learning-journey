{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GloVe and SWIVEL: Complete Word Embeddings Guide\n",
        "\n",
        "*Theory + Calculations + Q&A Format + Original Structure*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Co-occurrence Matrices\n",
        "\n",
        "### Q1: What is a co-occurrence matrix?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A co-occurrence matrix is a table that tracks which words appear near each other in text. Both rows and columns represent words from your vocabulary, and each cell contains a count of how many times those two words appeared together within a context window (typically 5 words apart).\n",
        "\n",
        "**Example Setup:**\n",
        "Vocabulary: {dog, bark, cat, meow, tree}\n",
        "\n",
        "| | dog | bark | cat | meow | tree |\n",
        "|---|---|---|---|---|---|\n",
        "| dog | 0 | 500 | 45 | 3 | 12 |\n",
        "| bark | 500 | 0 | 8 | 2 | 1 |\n",
        "| cat | 45 | 8 | 0 | 480 | 20 |\n",
        "| meow | 3 | 2 | 480 | 0 | 5 |\n",
        "| tree | 12 | 1 | 20 | 5 | 0 |\n",
        "\n",
        "**What this means:** The value 500 at (dog, bark) means \"In the corpus, 'dog' and 'bark' appear within a context window 500 times.\"\n",
        "\n",
        "**Important:** This matrix is symmetric—(dog, bark) = (bark, dog) = 500."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Why do we need a co-occurrence matrix?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Because of the principle: \"You shall know a word by the company it keeps.\" Words that frequently appear together are likely to be semantically related. The co-occurrence matrix captures these statistical relationships across the entire text corpus.\n",
        "\n",
        "**What this means:** If we can identify which words appear together often, we can infer semantic relationships. Words appearing with \"dog\" (like \"bark\", \"pet\", \"animal\") help us understand that \"dog\" is a noun related to animals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: The Problem with Large Matrices\n",
        "\n",
        "### Q3: Why is a 400,000 × 400,000 co-occurrence matrix a problem?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A vocabulary of 400,000 words creates a matrix with **160 billion cells**. This is computationally expensive to store and process. Most cells are zeros or very small numbers (sparse data), making this representation wasteful.\n",
        "\n",
        "**What this means:**\n",
        "- Storage: 160 billion numbers = huge memory requirement\n",
        "- Speed: Each similarity query requires looking up a cell in this massive matrix\n",
        "- Sparsity: Most word pairs don't co-occur, so most cells are zero (wasted space)\n",
        "- Example: If only 1 million word pairs actually co-occur, we have 159.999 billion zeros\n",
        "\n",
        "We need a better approach to represent this information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Matrix Factorization Solution\n",
        "\n",
        "### Q4: How do we solve the size problem?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "We use **matrix factorization**—specifically Singular Value Decomposition (SVD). The algorithm breaks down the massive 400,000 × 400,000 matrix into smaller, manageable matrices that can be multiplied back together to approximate the original.\n",
        "\n",
        "**What this means:** Instead of storing the full matrix, we store smaller matrices that, when multiplied, recreate approximately the same information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: How exactly does matrix factorization work?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The original co-occurrence matrix C is decomposed as:\n",
        "\n",
        "$$C = U \\Sigma V^T$$\n",
        "\n",
        "Where:\n",
        "- U is a 400,000 × r matrix (left singular vectors)\n",
        "- Σ is an r × r diagonal matrix of singular values\n",
        "- V^T is an r × 400,000 matrix (right singular vectors transposed)\n",
        "\n",
        "**What this means:**\n",
        "- Each singular value represents how much \"importance\" or \"variance\" that component captures\n",
        "- Most information concentrates in just a few singular values\n",
        "- We keep only the **k largest singular values** (e.g., k = 300) and set the rest to zero\n",
        "- This reduces storage and computation dramatically while preserving most information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: What do we end up with after factorization?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "After truncation to keep only 300 dimensions:\n",
        "- U becomes 400,000 × 300\n",
        "- Σ becomes 300 × 300\n",
        "- V^T becomes 300 × 400,000\n",
        "\n",
        "**Compression achieved:**\n",
        "- Original: 400,000 × 400,000 = 160 billion cells\n",
        "- Factorized: (400,000 × 300) + (300 × 300) + (300 × 400,000) ≈ 240 million cells\n",
        "- **Compression ratio: ~1,300x smaller**\n",
        "\n",
        "**What this means:** We've compressed the matrix dramatically while preserving semantic relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Why keep 400,000 in the dimension?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The 400,000 represents your **vocabulary size**—the number of unique words. Each word needs its own representation. The 300 represents the **embedding dimensions**—the compressed features that capture each word's semantic meaning.\n",
        "\n",
        "**What this means:**\n",
        "- 400,000: Each of the 400,000 words in vocabulary gets one row (their embedding)\n",
        "- 300: Each embedding is a vector of 300 numbers, not 400,000\n",
        "- Result: Each of the 400,000 words gets a 300-dimensional word embedding vector instead of a 400,000-dimensional co-occurrence vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Pointwise Mutual Information (PMI) - The Training Target\n",
        "\n",
        "### Q8: What is PMI and why do we need it?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "PMI (Pointwise Mutual Information) measures whether two words co-occur more or less often than random chance would predict. This is more meaningful than raw co-occurrence counts because it accounts for individual word frequencies.\n",
        "\n",
        "$$\\text{PMI}(w_1, w_2) = \\log\\left(\\frac{P(w_1, w_2)}{P(w_1) \\times P(w_2)}\\right)$$\n",
        "\n",
        "**What this means:**\n",
        "- **Numerator P(w₁, w₂):** Joint probability—both words appearing together\n",
        "- **Denominator P(w₁) × P(w₂):** Product of marginal probabilities—what would happen if words were independent\n",
        "- **Ratio:** How much more (or less) often they actually appear compared to random chance\n",
        "- **Log:** Compress ratio to log scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: Why not just use P(dog) × P(bark)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "P(dog) × P(bark) represents what we'd expect **if the words were independent**. It tells us what **random chance predicts**, not what's meaningful.\n",
        "\n",
        "**Two scenarios showing why this matters:**\n",
        "\n",
        "**Scenario 1: \"dog\" and \"bark\" (semantically related)**\n",
        "- Actual co-occurrence: 500 times\n",
        "- Expected by chance (P(dog) × P(bark)): ~250 times\n",
        "- Ratio: 500/250 = 2.0 (co-occur 2x more than chance predicts)\n",
        "- Conclusion: Strongly related ✓\n",
        "\n",
        "**Scenario 2: \"dog\" and \"the\" (NOT semantically related)**\n",
        "- Actual co-occurrence: 5,000 times (much higher count!)\n",
        "- Expected by chance: ~2,600 times\n",
        "- Ratio: 5000/2600 = 1.92 (co-occur about as often as chance predicts)\n",
        "- Conclusion: Not semantically related, just both common words ✓\n",
        "\n",
        "**The key insight:** Raw count (5,000 > 500) is misleading! PMI (dog-bark ≈ 2.0 > dog-the ≈ 1.92) correctly identifies dog-bark as more related.\n",
        "\n",
        "**Why divide instead of subtract?**\n",
        "- Subtraction doesn't scale well across different probability ranges\n",
        "- Example: Both (0.3/0.1 = 3.0) and (0.003/0.001 = 3.0) represent the same relationship (3x more frequent than chance)\n",
        "- But subtraction gives (0.3-0.1 = 0.2) vs (0.003-0.001 = 0.002), falsely suggesting they're different\n",
        "- Division preserves relative relationships regardless of absolute probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q10: How do we calculate PMI? Complete step-by-step for dog-meow\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Step 1: Count total unique co-occurrences**\n",
        "\n",
        "Sum only the unique pairs from upper triangle (matrix is symmetric, count each pair once):\n",
        "\n",
        "$$\\text{Total} = 500 + 45 + 3 + 12 + 8 + 2 + 1 + 480 + 20 + 5 = 1,076$$\n",
        "\n",
        "**What this means:** We observed 1,076 total unique word pair co-occurrences. (Not 2,000—we count each pair once, not symmetrically)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate joint probability P(dog, meow)**\n",
        "\n",
        "$$P(\\text{dog, meow}) = \\frac{3}{1076} = 0.0028$$\n",
        "\n",
        "**What this means:** Out of every 1,076 word pair co-occurrences, only 3 are \"dog\" with \"meow\". So 0.28% chance any random pair is dog-meow. (Very rare!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Calculate marginal probability P(dog)**\n",
        "\n",
        "Dog appears with: bark (500) + cat (45) + meow (3) + tree (12) = **560 total**\n",
        "\n",
        "$$P(\\text{dog}) = \\frac{560}{1076} = 0.520$$\n",
        "\n",
        "**What this means:** 52% of all pairs involve \"dog\". So \"dog\" is a fairly common word in co-occurrences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Calculate marginal probability P(meow)**\n",
        "\n",
        "Meow appears with: dog (3) + bark (2) + cat (480) + tree (5) = **490 total**\n",
        "\n",
        "$$P(\\text{meow}) = \\frac{490}{1076} = 0.455$$\n",
        "\n",
        "**What this means:** 45.5% of all pairs involve \"meow\". Slightly less common than \"dog\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Calculate probability if independent**\n",
        "\n",
        "If dog and meow appeared together purely by random chance:\n",
        "\n",
        "$$P(\\text{dog}) \\times P(\\text{meow}) = 0.520 \\times 0.455 = 0.237$$\n",
        "\n",
        "**What this means:** Expected them to co-occur with probability 0.237. Out of 1,076 pairs, we'd expect about 255 of them to be \"dog\" with \"meow\" (0.237 × 1,076 = 255).\n",
        "\n",
        "**Critical comparison:**\n",
        "- **Actual:** 3 times (P(dog, meow) = 0.0028)\n",
        "- **Expected by chance:** 255 times (P(dog) × P(meow) = 0.237)\n",
        "\n",
        "Dog and meow co-occur only 3/255 ≈ 0.012 times (1/84th) compared to random chance. They appear **far less** together than chance would predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Calculate the ratio**\n",
        "\n",
        "$$\\frac{P(\\text{dog, meow})}{P(\\text{dog}) \\times P(\\text{meow})} = \\frac{0.0028}{0.237} = 0.0118$$\n",
        "\n",
        "**What this means:** The ratio is 0.0118 (much less than 1.0):\n",
        "- Ratio > 1: Co-occur more than chance predicts (related)\n",
        "- Ratio = 1: Co-occur exactly as chance predicts (independent)\n",
        "- Ratio < 1: Co-occur less than chance predicts (unrelated)\n",
        "\n",
        "In this case, ratio = 0.0118 tells us dog and meow are **strongly unrelated**—they avoid each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 7: Apply logarithm to get PMI**\n",
        "\n",
        "$$\\text{PMI}(\\text{dog, meow}) = \\log(0.0118) = -4.74$$\n",
        "\n",
        "**What this means:**\n",
        "- Log converts ratio to log scale (makes numbers manageable and symmetric)\n",
        "- Negative PMI (-4.74) means words are **unrelated**\n",
        "- Large magnitude (|-4.74| is large) means **strong unrelatedness**\n",
        "- In embedding space, they should be **far apart** (even opposite directions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11: What does PMI value tell us?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- **PMI > 0:** Words co-occur MORE than chance predicts → Semantically related (attracted)\n",
        "- **PMI = 0:** Words co-occur EXACTLY as chance predicts → Independent, no relationship\n",
        "- **PMI < 0:** Words co-occur LESS than chance predicts → Semantically unrelated (avoid)\n",
        "\n",
        "For dog-meow: PMI ≈ -4.74 is very negative → strongly unrelated\n",
        "\n",
        "**What this means:** PMI(dog, meow) = -4.74 is our **training target**. When we train embeddings, we want:\n",
        "$$\\text{embedding}_{\\text{dog}} \\cdot \\text{embedding}_{\\text{meow}} \\approx -4.74$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Training Word Embeddings to Match PMI\n",
        "\n",
        "### Q12: What is GloVe's training objective?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "GloVe trains word embeddings so that the dot product between any two word vectors approximates their PMI value:\n",
        "\n",
        "$$\\text{embedding}_{w_1} \\cdot \\text{embedding}_{w_2} \\approx \\text{PMI}(w_1, w_2)$$\n",
        "\n",
        "**Important note:** This is a **design choice by GloVe engineers, not mathematically deduced.** They decided dot product should equal PMI because:\n",
        "- PMI already captures semantic relationships\n",
        "- Dot product is efficient and differentiable (works well with gradient descent)\n",
        "- It's simple and elegant\n",
        "\n",
        "This could have been done differently (e.g., dot product = raw count, or dot product = probability), but GloVe chose PMI.\n",
        "\n",
        "**What this means:** Each embedding value is discovered during optimization to satisfy this constraint. Values have no inherent meaning individually—only in combination via dot product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13: How does GloVe training work? Step-by-step\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Goal:** Make $\\text{embedding}_{\\text{dog}} \\cdot \\text{embedding}_{\\text{meow}} = -4.74$ (the PMI we calculated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1: Initialize random embeddings**\n",
        "\n",
        "```\n",
        "embedding_dog = [0.1, 0.2, 0.3, ..., 0.15]  (300 random numbers)\n",
        "embedding_meow = [0.1, 0.2, 0.3, ..., 0.20] (300 random numbers)\n",
        "```\n",
        "\n",
        "**What this means:** Start with random values. These will be optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate current dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.1 \\times 0.1) + (0.2 \\times 0.2) + (0.3 \\times 0.3) + \\ldots + (0.15 \\times 0.20) = 0.55$$\n",
        "\n",
        "**What this means:** Current result is 0.55, but we need -4.74. Error = 5.29 (huge!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Adjust embeddings**\n",
        "\n",
        "The optimization algorithm analyzes error and adjusts values. To make dot product negative, use negative values and opposite signs:\n",
        "\n",
        "```\n",
        "embedding_dog = [0.3, -0.5, 0.2, 0.1, 0.4, ..., -0.25]\n",
        "embedding_meow = [0.4, 0.3, 0.1, -0.2, 0.5, ..., 0.15]\n",
        "```\n",
        "\n",
        "**What this means:** Negative products like (-0.5 × 0.3) = -0.15 help achieve negative dot product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Recalculate dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.3 \\times 0.4) + (-0.5 \\times 0.3) + (0.2 \\times 0.1) + (0.1 \\times -0.2) + \\ldots = -2.1$$\n",
        "\n",
        "**What this means:** Better! Error reduced from 5.29 to 2.64. Still not -4.74, continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Repeat many iterations**\n",
        "\n",
        "| Iteration | Dot Product | Target | Error |\n",
        "|-----------|-------------|--------|-------|\n",
        "| 1 | 0.55 | -4.74 | 5.29 |\n",
        "| 2 | -2.1 | -4.74 | 2.64 |\n",
        "| 3 | -3.5 | -4.74 | 1.24 |\n",
        "| 100 | -4.70 | -4.74 | 0.04 |\n",
        "| Final | -4.74 | -4.74 | 0 ✓ |\n",
        "\n",
        "**What this means:** After thousands of iterations, we found values that produce exactly -4.74."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Final trained embeddings**\n",
        "\n",
        "```\n",
        "embedding_dog = [0.35, -0.85, 0.25, 0.05, 0.40, ..., -0.55]\n",
        "embedding_meow = [0.25, 0.65, 0.15, -0.45, 0.30, ..., 0.35]\n",
        "```\n",
        "\n",
        "**Verification:**\n",
        "$$\\text{dot product} = (0.35 \\times 0.25) + (-0.85 \\times 0.65) + (0.25 \\times 0.15) + \\ldots = 0.088 - 0.553 + 0.038 + \\ldots = -4.74$$ \n",
        "✓\n",
        "\n",
        "**What this means:** Embeddings are \"trained\"—values discovered by optimization. Notice they differ completely from random start. Optimizer found these numbers work together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14: Why doesn't each embedding dimension mean anything by itself?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Each value in an embedding is a parameter discovered during optimization. Its only purpose is to satisfy:\n",
        "\n",
        "> When dot-producted with another embedding, the result should equal the target PMI\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "embedding_dog[5] = 0.18\n",
        "```\n",
        "\n",
        "This 0.18 does NOT represent:\n",
        "- \"dog is 18% fierce\" ✗\n",
        "- \"dog has loyalty score 0.18\" ✗\n",
        "- \"feature X with strength 0.18\" ✗\n",
        "\n",
        "It's simply: \"A parameter that, when multiplied by embedding_meow[5] and combined with 299 other products, helps achieve the target dot product of -4.74\" ✓\n",
        "\n",
        "**Semantic meaning emerges from the COMBINATION via dot product, not from individual values.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15: Why is \"loosely defined\" better than pre-defining what each dimension means?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Alternative: Pre-define semantic features**\n",
        "```python\n",
        "embedding_dog[0] = compute_animality_score(\"dog\")\n",
        "embedding_dog[1] = compute_domesticity_score(\"dog\")\n",
        "embedding_dog[2] = compute_loyalty_score(\"dog\")\n",
        "```\n",
        "\n",
        "**Problem:** How do you compute these scores? There's no formula. This approach fails.\n",
        "\n",
        "**GloVe's approach: Let optimizer discover values**\n",
        "- No need to pre-define what dimensions mean\n",
        "- Just optimize to match PMI targets\n",
        "- Result: Flexible, universal embeddings\n",
        "- Works for any language, domain, task\n",
        "- Emerges from data, not human assumptions\n",
        "\n",
        "**Why it works:** PMI structure naturally encodes semantic relationships. Optimization finds 300-dim representation that preserves this. Even though individual dimensions are \"meaningless\", together they capture semantic meaning via dot products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Using the Trained Embeddings\n",
        "\n",
        "### Q16: How do we approximate co-occurrence relationships from trained embeddings?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "After training, use the **dot product** between vectors to approximate **PMI**:\n",
        "\n",
        "$$\\text{dot product}(\\text{embedding}_{\\text{dog}}, \\text{embedding}_{\\text{meow}}) \\approx \\text{PMI}(\\text{dog, meow}) = -4.74$$\n",
        "\n",
        "**With trained embeddings:**\n",
        "- embedding_dog = [0.35, -0.85, 0.25, 0.05, 0.40, ..., -0.55] (300 numbers)\n",
        "- embedding_meow = [0.25, 0.65, 0.15, -0.45, 0.30, ..., 0.35] (300 numbers)\n",
        "\n",
        "**The dot product:**\n",
        "$$\\text{dot product} = (0.35 \\times 0.25) + (-0.85 \\times 0.65) + \\ldots = -4.74$$\n",
        "\n",
        "**Interpretation:**\n",
        "- Negative dot product (-4.74) → Unrelated words\n",
        "- Positive dot product → Related words\n",
        "- Magnitude → Strength of relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17: Can we recover original counts from embeddings?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Yes, reverse the PMI calculation:\n",
        "\n",
        "**Starting:** Trained embeddings give dot product = -4.74\n",
        "\n",
        "**Step 1:** Exponentiate\n",
        "$$e^{-4.74} = 0.0087 = \\frac{P(\\text{dog, meow})}{P(\\text{dog}) \\times P(\\text{meow})}$$\n",
        "\n",
        "**Step 2:** Multiply by marginals\n",
        "$$0.0087 \\times 0.520 \\times 0.455 = 0.00206 \\approx P(\\text{dog, meow})$$\n",
        "\n",
        "**Step 3:** Multiply by total\n",
        "$$0.00206 \\times 1076 \\approx 2.2 \\approx 3$$    \n",
        "✓ Original count\n",
        "\n",
        "**What this means:** You can recover original information, but you don't need to. Dot product directly gives the PMI (semantic similarity), which is what matters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q18: Why does GloVe work?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Why GloVe Works:**\n",
        "\n",
        "1. **Compression:** 300 dimensions capture essential semantic information from 400,000-dim original\n",
        "2. **Efficiency:** Computing similarity is simple dot product, not matrix lookup\n",
        "3. **Semantic meaning:** Dot product reflects meaningful relationships via PMI\n",
        "4. **Learned representations:** Values discovered through optimization\n",
        "5. **Generalization:** Embeddings work for tasks beyond co-occurrence\n",
        "\n",
        "**Limitation:** GloVe only trains on observed (non-zero) pairs. Rare words get few training signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19: What is SWIVEL and how does it compare with GloVe?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "SWIVEL also trains embeddings to match PMI, but includes **zero-count pairs** in training.\n",
        "\n",
        "**SWIVEL Training Example: dog-meow**\n",
        "\n",
        "Same calculation as GloVe:\n",
        "$$\\text{Target PMI} = \\log\\left(\\frac{0.0028}{0.237}\\right) = \\log(0.0118) = -4.74$$\n",
        "\n",
        "**Loss function for non-zero pairs:**\n",
        "$$\\text{Loss}_{\\text{non-zero}} = f(\\text{count}) \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "where f(3) = √3 ≈ 1.7\n",
        "\n",
        "**Example:**\n",
        "- Dog-meow: Loss = 1.7 × (0.55 - (-4.74))² = 1.7 × 27.98 ≈ 47.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additionally, SWIVEL processes zero-count pairs:**\n",
        "\n",
        "**Loss function for zero-count pairs:**\n",
        "$$\\text{Loss}_{\\text{zero}} = 0.75 \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "**Example (dog-xylophone never co-occur):**\n",
        "- Target PMI ≈ -5 (large negative)\n",
        "- Random dot product ≈ 0.1\n",
        "- Loss = 0.75 × (0.1 - (-5))² = 0.75 × 26.01 ≈ 19.5\n",
        "\n",
        "**What this means:** SWIVEL has many more training constraints (both observed + unobserved) that regularize embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20: GloVe vs SWIVEL Step-by-Step Comparison\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Training the pair dog-meow (count=3, target PMI=-4.74):**\n",
        "\n",
        "| Step | GloVe | SWIVEL | Note |\n",
        "|------|-------|--------|------|\n",
        "| **Pair type** | Non-zero: dog-meow | Non-zero: dog-meow | Both process observed pairs |\n",
        "| **Count** | 3 | 3 | Same |\n",
        "| **Target PMI** | -4.74 | -4.74 | Same |\n",
        "| **Weight function** | Implicit | f(3) = √3 ≈ 1.7 | SWIVEL weights by √count |\n",
        "| **Error (iter 1)** | (0.55-(-4.74))² = 27.98 | Same | Same error |\n",
        "| **Loss (iter 1)** | 27.98 | 1.7 × 27.98 ≈ 47.6 | SWIVEL: Higher weighted |\n",
        "| **Add. constraints** | None for zeros | Many zeros! | SWIVEL: ~399,995 zero pairs for rare words |\n",
        "| **Zero-pair loss** | N/A (ignored) | 0.75 × (0.1-(-5))² ≈ 19.5 | Small weight prevents overwhelming |\n",
        "| **Final result** | dot ≈ -4.74 | dot ≈ -4.74 | Both achieve target |\n",
        "| **Rare word quality** | Poor (few constraints) | Excellent (many constraints) | **SWIVEL significantly better** |\n",
        "\n",
        "**Key difference:** SWIVEL adds thousands of zero-pair constraints that regularize rare word embeddings, while GloVe ignores zero pairs entirely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21: Piecewise Loss Function Explained\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "SWIVEL uses different weights for different pair types:\n",
        "\n",
        "**For non-zero-count pairs:**\n",
        "$$\\text{Loss} = f(\\text{count}) \\times (\\text{error})^2 \\text{ where } f(\\text{count}) = \\sqrt{\\text{count}}$$\n",
        "\n",
        "**What this means:**\n",
        "- f(500) = √500 ≈ 22.4 (high-count pairs weighted heavily)\n",
        "- f(3) = √3 ≈ 1.7 (low-count pairs weighted lightly)\n",
        "- Intuition: Confident about frequent co-occurrences, less confident about rare ones\n",
        "\n",
        "**For zero-count pairs:**\n",
        "$$\\text{Loss} = 0.75 \\times (\\text{error})^2$$\n",
        "\n",
        "**What this means:**\n",
        "- Small constant weight (0.75)\n",
        "- Zero-count pairs are billions (overwhelm if equal weight)\n",
        "- Small weight provides regularization without domination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22: Why is SWIVEL better for rare words?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Scenario: Training \"xylophone\" (appears 5 times)**\n",
        "\n",
        "**GloVe:**\n",
        "- Trains on ~5 non-zero pairs (observed co-occurrences)\n",
        "- Only 5 training constraints\n",
        "- Under-constrained: Many possible placements\n",
        "- Result: Embedding might be random\n",
        "\n",
        "**SWIVEL:**\n",
        "- Trains on ~5 non-zero pairs\n",
        "- PLUS ~399,995 zero-count pairs\n",
        "- Thousands of constraints saying: \"xylophone should be far from unrelated words\"\n",
        "- Well-constrained: Meaningful placement\n",
        "- Result: Significantly better rare word embeddings\n",
        "\n",
        "**Empirical result:** SWIVEL outperforms GloVe on rare word benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "1. **Co-occurrence matrices** capture semantic relationships but are too large (160 billion cells)\n",
        "\n",
        "2. **Matrix factorization** compresses to 300 dimensions (~1,300x smaller)\n",
        "\n",
        "3. **PMI** measures semantic relationships more accurately than raw counts\n",
        "\n",
        "4. **GloVe** trains embeddings to match PMI values via dot product (design choice, not mathematical necessity)\n",
        "\n",
        "5. **Embedding values** are discovered parameters with no individual meaning—semantic meaning emerges from dot products\n",
        "\n",
        "6. **SWIVEL** improves on GloVe by including zero-count pairs, significantly better for rare words\n",
        "\n",
        "7. **Piecewise loss** balances high-count pairs (heavy weight) with zero-count pairs (light weight)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
