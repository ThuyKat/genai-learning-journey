{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GloVe and SWIVEL Word Embeddings: Complete Q&A Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Co-occurrence Matrices\n",
        "\n",
        "### Q1: What is a co-occurrence matrix?\n",
        "\n",
        "A co-occurrence matrix is a table that tracks which words appear near each other in text. Both rows and columns represent words from your vocabulary, and each cell contains a count of how many times those two words appeared together within a context window (typically 5 words apart).\n",
        "\n",
        "**Example Setup:**\n",
        "Vocabulary: {dog, bark, cat, meow, tree}\n",
        "\n",
        "| | dog | bark | cat | meow | tree |\n",
        "|---|---|---|---|---|---|\n",
        "| dog | 0 | 500 | 45 | 3 | 12 |\n",
        "| bark | 500 | 0 | 8 | 2 | 1 |\n",
        "| cat | 45 | 8 | 0 | 480 | 20 |\n",
        "| meow | 3 | 2 | 480 | 0 | 5 |\n",
        "| tree | 12 | 1 | 20 | 5 | 0 |\n",
        "\n",
        "The value 500 at (dog, bark) means: \"In the corpus, 'dog' and 'bark' appear within a context window 500 times.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Why do we need a co-occurrence matrix?\n",
        "\n",
        "Because of the principle: \"You shall know a word by the company it keeps.\" Words that frequently appear together are likely to be semantically related. The co-occurrence matrix captures these statistical relationships across the entire text corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: The Problem with Large Matrices\n",
        "\n",
        "### Q3: Why is a 400,000 \u00d7 400,000 co-occurrence matrix a problem?\n",
        "\n",
        "A vocabulary of 400,000 words creates a matrix with **160 billion cells**. This is computationally expensive to store and process. Most cells are zeros or very small numbers (sparse data), making this representation wasteful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Matrix Factorization Solution\n",
        "\n",
        "### Q4: How do we solve the size problem?\n",
        "\n",
        "We use **matrix factorization**\u2014specifically Singular Value Decomposition (SVD). The algorithm breaks down the massive 400,000 \u00d7 400,000 matrix into smaller, manageable matrices that can be multiplied back together to approximate the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: How exactly does matrix factorization work?\n",
        "\n",
        "The original co-occurrence matrix C is decomposed as:\n",
        "\n",
        "$$C = U \\Sigma V^T$$\n",
        "\n",
        "Where:\n",
        "- U is a 400,000 \u00d7 r matrix\n",
        "- \u03a3 is an r \u00d7 r diagonal matrix of singular values\n",
        "- V^T is an r \u00d7 400,000 matrix\n",
        "\n",
        "The key insight: most information concentrates in just a few singular values. We keep only the **k largest singular values** (e.g., k = 300) and set the rest to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: What do we end up with after factorization?\n",
        "\n",
        "After truncation to keep only 300 dimensions:\n",
        "- U becomes 400,000 \u00d7 300\n",
        "- \u03a3 becomes 300 \u00d7 300\n",
        "- V^T becomes 300 \u00d7 400,000\n",
        "\n",
        "We've compressed **160 billion cells down to approximately 240 million cells total**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Why keep 400,000 in the dimension?\n",
        "\n",
        "The 400,000 represents your **vocabulary size**\u2014the number of unique words. Each word needs its own representation. The 300 represents the **embedding dimensions**\u2014the compressed features that capture each word's semantic meaning.\n",
        "\n",
        "**Result:** Each of the 400,000 words gets a 300-dimensional word embedding vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Pointwise Mutual Information (PMI) - The Training Target\n",
        "\n",
        "**IMPORTANT:** Before we can train embeddings, we must first calculate the PMI values from the co-occurrence matrix. These PMI values become the targets that the embeddings are trained to match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8: What is PMI and why do we need it?\n",
        "\n",
        "PMI (Pointwise Mutual Information) measures whether two words co-occur more or less often than random chance would predict. This is more meaningful than raw co-occurrence counts because it accounts for individual word frequencies.\n",
        "\n",
        "$$\\text{PMI}(\\text{dog, bark}) = \\log\\left(\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: Why not just use P(dog) \u00d7 P(bark)?\n",
        "\n",
        "Great question! Let me explain why we need the full PMI formula instead of just multiplying the marginal probabilities.\n",
        "\n",
        "**The Problem with Just P(dog) \u00d7 P(bark):**\n",
        "\n",
        "If we only used P(dog) \u00d7 P(bark), we'd be calculating the probability that dog and bark co-occur **if they were completely independent** (if their appearance had nothing to do with each other).\n",
        "\n",
        "**Example to show why this matters:**\n",
        "\n",
        "Imagine two scenarios:\n",
        "\n",
        "**Scenario 1: \"dog\" and \"bark\" are semantically related**\n",
        "- They naturally appear together because they're related concepts\n",
        "- Actual co-occurrence: 500 times\n",
        "- If they were random/independent: would only appear together ~146 times (based on P(dog) \u00d7 P(bark))\n",
        "- Result: They appear together **much more than chance** predicts\n",
        "\n",
        "**Scenario 2: \"dog\" and \"the\" are not semantically related**\n",
        "- \"the\" is a common word that appears everywhere\n",
        "- Actual co-occurrence: 5,000 times (very high count!)\n",
        "- If they were independent: would appear together ~2,800 times\n",
        "- Result: They appear together roughly as much as chance would predict\n",
        "\n",
        "**The key insight:** Raw co-occurrence counts are misleading. \"dog\" and \"the\" co-occur more frequently (5,000 vs 500), but \"dog\" and \"bark\" are more semantically related. We need PMI to distinguish this difference.\n",
        "\n",
        "PMI fixes this by comparing:\n",
        "- **Actual:** How often they really co-occur\n",
        "- **Expected by chance:** P(dog) \u00d7 P(bark)\n",
        "\n",
        "If actual > expected, PMI is positive (they're attracted to each other semantically).\n",
        "If actual = expected, PMI is zero (they're independent).\n",
        "If actual < expected, PMI is negative (they avoid each other)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Calculate PMI Step-by-Step\n",
        "\n",
        "### Q10: How do we calculate PMI step-by-step?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1: Calculate total co-occurrences**\n",
        "\n",
        "Sum all values in the co-occurrence matrix (excluding diagonal):\n",
        "Total = 500 + 45 + 3 + 12 + 500 + 8 + 2 + 1 + 45 + 8 + 480 + 20 + 3 + 2 + 480 + 5 + 12 + 1 + 20 + 5 = 2,000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate the joint probability P(dog, bark)**\n",
        "\n",
        "$$P(\\text{dog, bark}) = \\frac{\\text{co-occurrence count}}{\\text{total co-occurrences}} = \\frac{500}{2000} = 0.25$$\n",
        "\n",
        "This is the probability that any randomly selected co-occurrence involves both \"dog\" and \"bark.\"\n",
        "\n",
        "**What this means:** Out of every 2,000 word pair co-occurrences we observe, 500 of them are \"dog\" paired with \"bark.\" So there's a 25% chance any randomly selected co-occurrence is this pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Calculate marginal probability for dog**\n",
        "\n",
        "Dog appears in co-occurrences with: bark (500) + cat (45) + meow (3) + tree (12) = 560 times\n",
        "\n",
        "$$P(\\text{dog}) = \\frac{560}{2000} = 0.28$$\n",
        "\n",
        "This is the probability that any co-occurrence involves \"dog\" (regardless of what it co-occurs with).\n",
        "\n",
        "**What this means:** Out of every 2,000 co-occurrences, 560 of them involve the word \"dog\" (paired with anything). So there's a 28% chance any randomly selected co-occurrence includes \"dog.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Calculate marginal probability for bark**\n",
        "\n",
        "Bark appears in co-occurrences with: dog (500) + cat (8) + meow (2) + tree (1) = 511 times\n",
        "\n",
        "$$P(\\text{bark}) = \\frac{511}{2000} = 0.26$$\n",
        "\n",
        "This is the probability that any co-occurrence involves \"bark\" (regardless of what it co-occurs with).\n",
        "\n",
        "**What this means:** Out of every 2,000 co-occurrences, 511 of them involve the word \"bark\" (paired with anything). So there's a 26% chance any randomly selected co-occurrence includes \"bark.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Calculate the probability if they were independent**\n",
        "\n",
        "If \"dog\" and \"bark\" appeared together purely by random chance (completely independent), the probability would be:\n",
        "\n",
        "$$P(\\text{dog}) \\times P(\\text{bark}) = 0.28 \\times 0.26 = 0.073$$\n",
        "\n",
        "**What this means:** If dog and bark had nothing to do with each other\u2014if they just randomly happened to appear in the same context windows\u2014they'd co-occur with probability 0.073. Out of 2,000 co-occurrences, we'd expect about 146 of them to be \"dog\" with \"bark\" (0.073 \u00d7 2,000 = 146).\n",
        "\n",
        "**This is the crucial comparison:**\n",
        "- **Actual co-occurrence:** 500 times (P(dog, bark) = 0.25)\n",
        "- **Expected by chance:** 146 times (P(dog) \u00d7 P(bark) = 0.073)\n",
        "\n",
        "Dog and bark co-occur 500 \u00f7 146 = 3.42 times MORE OFTEN than we'd expect if they were independent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Calculate the ratio - Why do we divide?**\n",
        "\n",
        "Now we form the ratio to see how much MORE frequently they co-occur than chance predicts:\n",
        "\n",
        "$$\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})} = \\frac{0.25}{0.073} = 3.42$$\n",
        "\n",
        "**This ratio tells us:**\n",
        "- If ratio > 1: They co-occur MORE often than chance predicts (they're attracted to each other)\n",
        "- If ratio = 1: They co-occur EXACTLY as often as chance would predict (they're independent)\n",
        "- If ratio < 1: They co-occur LESS often than chance predicts (they avoid each other)\n",
        "\n",
        "In our case, the ratio is 3.42, meaning dog and bark co-occur **3.42 times more often than random chance would predict**.\n",
        "\n",
        "**Why divide instead of subtract?**\n",
        "\n",
        "You might ask: \"Why not just calculate P(dog, bark) - P(dog) \u00d7 P(bark) = 0.25 - 0.073 = 0.177?\"\n",
        "\n",
        "Answer: Because subtraction doesn't scale well with different probability ranges.\n",
        "\n",
        "**Example:**\n",
        "- Scenario A: Actual = 0.3, Expected = 0.1 \u2192 Difference = 0.2 \u2192 Ratio = 3.0\n",
        "- Scenario B: Actual = 0.003, Expected = 0.001 \u2192 Difference = 0.002 \u2192 Ratio = 3.0\n",
        "\n",
        "Both scenarios have the same ratio (3.0 times more frequent than expected), but different differences (0.2 vs 0.002). Division preserves the relative relationship regardless of absolute probability values. Subtraction would incorrectly suggest Scenario A is more significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 7: Apply logarithm to get PMI**\n",
        "\n",
        "$$\\text{PMI}(\\text{dog, bark}) = \\log(3.42) \\approx 1.23$$\n",
        "\n",
        "We apply logarithm because:\n",
        "1. It compresses large ratio values (prevents extreme numbers)\n",
        "2. It makes ratios comparable on a symmetric scale (log(3) and log(1/3) have equal magnitude but opposite signs)\n",
        "3. It's mathematically convenient for optimization algorithms\n",
        "\n",
        "**What the 1.23 value means:**\n",
        "- It's the log of how much more often they co-occur than chance predicts\n",
        "- Positive 1.23 means they're strongly semantically related\n",
        "- The magnitude tells us the strength of the relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11: What does this PMI value tell us?\n",
        "\n",
        "- **PMI > 0:** The words co-occur more often than random chance predicts. They're semantically related and attracted to each other.\n",
        "- **PMI = 0:** They co-occur exactly as random chance would predict. No meaningful relationship.\n",
        "- **PMI < 0:** They co-occur less than random chance predicts. They tend to avoid each other.\n",
        "\n",
        "For our example, PMI \u2248 1.23 is positive and relatively large, meaning \"dog\" and \"bark\" are genuinely semantically related\u2014not just appearing together by coincidence.\n",
        "\n",
        "**This 1.23 value is now our training target for the embeddings.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Training Word Embeddings to Match PMI\n",
        "\n",
        "### Q12: What is GloVe's training objective?\n",
        "\n",
        "GloVe trains word embeddings so that the dot product between any two word vectors approximates their PMI value:\n",
        "\n",
        "$$\\text{dot product}(\\text{embedding}_{\\text{dog}}, \\text{embedding}_{\\text{bark}}) \\approx \\text{PMI}(\\text{dog, bark}) = 1.23$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13: How does the training process work?\n",
        "\n",
        "**Step 1: Initialize random embeddings**\n",
        "\n",
        "Start with random 300-dimensional vectors for each word:\n",
        "- embedding_dog = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.15] (300 random numbers)\n",
        "- embedding_bark = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.20] (300 random numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate current dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.1 \\times 0.1) + (0.2 \\times 0.2) + (0.3 \\times 0.3) + \\ldots + (0.15 \\times 0.20)$$\n",
        "\n",
        "Let's say this equals 0.55."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Compare to target PMI**\n",
        "\n",
        "- Target: 1.23\n",
        "- Current: 0.55\n",
        "- Error: 1.23 - 0.55 = 0.68 (too low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Adjust the embeddings**\n",
        "\n",
        "The optimization algorithm (like gradient descent) adjusts the numbers in both embeddings to reduce the error. It might change:\n",
        "- embedding_dog to [0.3, 0.15, 0.5, 0.2, 0.4, ..., 0.25]\n",
        "- embedding_bark to [0.4, 0.2, 0.6, 0.25, 0.5, ..., 0.30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Recalculate dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.3 \\times 0.4) + (0.15 \\times 0.2) + (0.5 \\times 0.6) + \\ldots + (0.25 \\times 0.30)$$\n",
        "\n",
        "Now equals 0.70. Still not 1.23, so continue adjusting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Repeat iteratively**\n",
        "\n",
        "The algorithm continues adjusting the embedding values thousands of times. After many iterations, it might reach:\n",
        "- embedding_dog = [0.48, -0.25, 0.70, 0.14, 0.58, ..., -0.34]\n",
        "- embedding_bark = [0.55, -0.22, 0.74, 0.10, 0.52, ..., -0.29]\n",
        "\n",
        "$$\\text{dot product} = (0.48 \\times 0.55) + (-0.25 \\times -0.22) + (0.70 \\times 0.74) + \\ldots + (-0.34 \\times -0.29)$$\n",
        "$$= 0.264 + 0.055 + 0.518 + \\ldots + 0.099$$\n",
        "$$= 1.23$$\n",
        "\n",
        "**Success!** The dot product now matches the target PMI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14: What do these embedding numbers represent?\n",
        "\n",
        "The numbers in the embeddings (like 0.48, -0.25, 0.70, etc.) are **learned parameters discovered through optimization**. They are:\n",
        "\n",
        "- **NOT** co-occurrence counts from the matrix\n",
        "- **NOT** probabilities\n",
        "- **NOT** logarithms of anything\n",
        "- **NOT** mathematically derived from a formula\n",
        "\n",
        "They are simply **values that the algorithm discovered through trial and error** to satisfy the constraint that their dot product equals the target PMI.\n",
        "\n",
        "Think of it like solving a puzzle:\n",
        "- **The puzzle:** Find two sets of 300 numbers that multiply together (dot product) to give 1.23\n",
        "- **The optimizer:** Tries different number combinations repeatedly and measures the error\n",
        "- **The solution:** After many iterations, finds one valid combination\n",
        "\n",
        "There's nothing inherently special about the specific values [0.48, -0.25, 0.70, ...]. A different optimization run might produce different numbers. **As long as the dot product equals 1.23, any set of numbers works.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15: Does GloVe train embeddings for just one word pair?\n",
        "\n",
        "No! GloVe simultaneously trains embeddings to satisfy the PMI constraint for **all word pairs** in the vocabulary.\n",
        "\n",
        "For our 5-word vocabulary, it needs to satisfy:\n",
        "- dot product(dog, bark) \u2248 1.23\n",
        "- dot product(dog, cat) \u2248 PMI(dog, cat)\n",
        "- dot product(dog, meow) \u2248 PMI(dog, meow)\n",
        "- dot product(dog, tree) \u2248 PMI(dog, tree)\n",
        "- dot product(bark, cat) \u2248 PMI(bark, cat)\n",
        "- ...and all other pairs\n",
        "\n",
        "The algorithm adjusts all embeddings simultaneously to minimize the total error across all word pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Using the Trained Embeddings\n",
        "\n",
        "### Q16: How do we approximate co-occurrence relationships from trained embeddings?\n",
        "\n",
        "After training completes, we can use the **dot product** between two word embedding vectors to approximate their **PMI value**:\n",
        "\n",
        "$$\\text{dot product}(\\text{embedding}_{\\text{dog}}, \\text{embedding}_{\\text{bark}}) \\approx \\text{PMI}(\\text{dog, bark}) = 1.23$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**With the trained embeddings:**\n",
        "- embedding_dog = [0.48, -0.25, 0.70, 0.14, 0.58, ..., -0.34] (300 numbers)\n",
        "- embedding_bark = [0.55, -0.22, 0.74, 0.10, 0.52, ..., -0.29] (300 numbers)\n",
        "\n",
        "**The dot product calculation:**\n",
        "\n",
        "$$\\text{dot product} = (0.48 \\times 0.55) + (-0.25 \\times -0.22) + (0.70 \\times 0.74) + (0.14 \\times 0.10) + (0.58 \\times 0.52) + \\ldots + (-0.34 \\times -0.29)$$\n",
        "\n",
        "$$= 0.264 + 0.055 + 0.518 + 0.014 + 0.302 + \\ldots + 0.099$$\n",
        "\n",
        "$$= 1.23$$\n",
        "\n",
        "This 1.23 tells us that \"dog\" and \"bark\" are strongly semantically related (positive PMI = they co-occur more than chance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17: Can we get back the original co-occurrence count?\n",
        "\n",
        "Yes, but it requires reversing the PMI calculation:\n",
        "\n",
        "**Step 1: Exponentiate the dot product (PMI)**\n",
        "\n",
        "$$e^{1.23} = 3.42$$\n",
        "\n",
        "This gives us the ratio $\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Multiply by marginal probabilities**\n",
        "\n",
        "$$3.42 \\times 0.28 \\times 0.26 = 0.249 \\approx 0.25$$\n",
        "\n",
        "This gives us P(dog, bark)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Multiply by total co-occurrences**\n",
        "\n",
        "$$0.25 \\times 2000 = 500$$\n",
        "\n",
        "We've recovered the original co-occurrence count from the co-occurrence matrix!\n",
        "\n",
        "**However, in practice, you rarely need to do this.** For most NLP tasks, you only need the PMI (semantic similarity), which the dot product gives you directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Complete Summary of GloVe\n",
        "\n",
        "### The GloVe Process, Step by Step\n",
        "\n",
        "**Phase 1: Build the co-occurrence matrix**\n",
        "Count how often words appear together in the corpus.\n",
        "- Result: 400,000 \u00d7 400,000 matrix with 160 billion cells\n",
        "- Example: dog and bark co-occur 500 times\n",
        "\n",
        "**Phase 2: Calculate PMI values for all word pairs**\n",
        "For each word pair, calculate how much more frequently they co-occur than chance would predict.\n",
        "- Calculate P(dog, bark), P(dog), P(bark)\n",
        "- Calculate ratio: P(dog, bark) / (P(dog) \u00d7 P(bark)) = 0.25 / 0.073 = 3.42\n",
        "- Apply log: log(3.42) = 1.23\n",
        "- Result: PMI targets for all word pairs\n",
        "- Example: PMI(dog, bark) = 1.23\n",
        "\n",
        "**Phase 3: Initialize random embeddings**\n",
        "Start with random 300-dimensional vectors for each word.\n",
        "- Result: 400,000 \u00d7 300 word embedding matrix with random values\n",
        "\n",
        "**Phase 4: Train embeddings via optimization**\n",
        "Iteratively adjust embedding values so that dot products match PMI targets.\n",
        "- Optimization method: Gradient descent or alternating least squares\n",
        "- Objective: Minimize error between dot products and PMI values across all word pairs\n",
        "- Result: Trained embeddings where dot product(dog, bark) \u2248 1.23\n",
        "\n",
        "**Phase 5: Use embeddings**\n",
        "After training, compute semantic relationships using simple dot products.\n",
        "- Compression: 300 dimensions instead of 400,000\n",
        "- Storage: 240 million cells instead of 160 billion cells\n",
        "- Query: Simple dot product instead of matrix lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why GloVe Works\n",
        "\n",
        "- **Compression:** 300 dimensions capture the essential semantic information that would require 400,000 dimensions in the original matrix.\n",
        "- **Efficiency:** Computing similarity is a simple dot product, not a matrix lookup.\n",
        "- **Semantic meaning:** The dot product reflects meaningful semantic relationships captured by PMI, not just raw frequency.\n",
        "- **Learned representations:** The embedding values are discovered through optimization to satisfy PMI constraints across all word pairs simultaneously.\n",
        "- **Generalization:** Embeddings work for tasks beyond just co-occurrence because they encode deep semantic structure in the learned parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: SWIVEL - An Alternative Approach\n",
        "\n",
        "### Q18: What is SWIVEL and how does it differ from GloVe?\n",
        "\n",
        "SWIVEL stands for **Submatrix-wise Vector Embedding Learner**. Like GloVe, it's a count-based method that learns word embeddings from a co-occurrence matrix, but it uses a different approach:\n",
        "\n",
        "**Key differences:**\n",
        "- **GloVe:** Trains only on *observed* co-occurrences (word pairs that appear together)\n",
        "- **SWIVEL:** Trains on *both* observed AND *unobserved* co-occurrences (word pairs that DON'T appear together)\n",
        "\n",
        "Both methods approximate the PMI matrix, but SWIVEL's special handling of unobserved co-occurrences makes it more accurate on rare words. However, SWIVEL is also considerably faster to train than GloVe in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19: Why does considering unobserved co-occurrences matter?\n",
        "\n",
        "Let me illustrate with our dog/bark/cat/meow/tree example.\n",
        "\n",
        "**The co-occurrence matrix (observed pairs only):**\n",
        "\n",
        "| | dog | bark | cat | meow | tree |\n",
        "|---|---|---|---|---|---|\n",
        "| dog | 0 | 500 | 45 | 3 | 12 |\n",
        "| bark | 500 | 0 | 8 | 2 | 1 |\n",
        "| cat | 45 | 8 | 0 | 480 | 20 |\n",
        "| meow | 3 | 2 | 480 | 0 | 5 |\n",
        "| tree | 12 | 1 | 20 | 5 | 0 |\n",
        "\n",
        "Notice the diagonal is all zeros (words don't co-occur with themselves by definition).\n",
        "\n",
        "**What GloVe does:**\n",
        "GloVe only trains on the 10 non-zero cells (the 5 observed pairs, counted twice symmetrically). It completely ignores:\n",
        "- The 5 diagonal cells (dog-dog, bark-bark, etc.)\n",
        "- Any cells that could be zero due to the corpus structure\n",
        "\n",
        "This means GloVe has no constraint on where to place unrelated words' embeddings. If two words never co-occur, GloVe doesn't care if their embeddings point in similar or opposite directions\u2014there's no training signal telling it to push them apart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20: How does SWIVEL handle unobserved co-occurrences?\n",
        "\n",
        "SWIVEL treats the entire matrix differently:\n",
        "\n",
        "**For observed co-occurrences (like dog-bark = 500):**\n",
        "- Target PMI: log(0.25 / 0.073) = 1.23\n",
        "- Train embedding_dog \u00b7 embedding_bark to equal 1.23\n",
        "- Use strong penalty if the dot product differs significantly\n",
        "\n",
        "**For unobserved co-occurrences (like dog-meow = 3, or any pair with very low counts):**\n",
        "- SWIVEL expects PMI to be low or negative (they're rare or unrelated)\n",
        "- Instead of ignoring it, SWIVEL penalizes if the dot product is HIGH\n",
        "- This pushes unrelated embeddings away from each other\n",
        "\n",
        "**Example:** If dog and meow co-occur only 3 times:\n",
        "- Total co-occurrences: 2,000\n",
        "- P(dog, meow) = 3 / 2,000 = 0.0015\n",
        "- P(dog) = 0.28, P(meow) = (3 + 2 + 480 + 5) / 2,000 = 0.245\n",
        "- P(dog) \u00d7 P(meow) = 0.28 \u00d7 0.245 = 0.0686\n",
        "- Ratio = 0.0015 / 0.0686 = 0.022 (much less than 1!)\n",
        "- PMI = log(0.022) \u2248 -3.82\n",
        "\n",
        "GloVe ignores this constraint. **SWIVEL enforces it:** The dot product should be approximately -3.82, not positive. This pushes the dog and meow embeddings in opposite directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21: What is the piecewise loss function in SWIVEL?\n",
        "\n",
        "SWIVEL uses different loss calculations depending on whether a co-occurrence is observed or unobserved:\n",
        "\n",
        "**For observed co-occurrences (high confidence):**\n",
        "$$\\text{Loss}_{\\text{observed}} = f(\\text{count}) \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "Where $$f(\\text{count})$$ is a weighting function that:\n",
        "- Gives higher weight to co-occurrences with higher counts\n",
        "- This makes sense: if two words appear together 500 times, we're very confident they're related\n",
        "- If they appear together only 2 times, we're less certain\n",
        "\n",
        "**For unobserved co-occurrences (zero count):**\n",
        "$$\\text{Loss}_{\\text{unobserved}} = c \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "Where $$c$$ is a small constant (like 0.75).\n",
        "\n",
        "**Why the difference?**\n",
        "- Unobserved pairs are very numerous (most word pairs in the vocabulary don't co-occur)\n",
        "- If we weighted them equally, they'd overwhelm the training signal from observed pairs\n",
        "- By using a smaller weight $$c$$, SWIVEL balances the learning:\n",
        "  - Learned relationships are primarily driven by observed co-occurrences\n",
        "  - But unobserved co-occurrences provide regularization to prevent rare words from being placed randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22: Let's compare SWIVEL and GloVe training on dog/bark example\n",
        "\n",
        "**Starting point:** Same random embeddings as GloVe\n",
        "- embedding_dog = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.15] (300 random numbers)\n",
        "- embedding_bark = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.20] (300 random numbers)\n",
        "- Current dot product = 0.55\n",
        "\n",
        "**GloVe training:**\n",
        "- Sees the pair (dog, bark) with count = 500\n",
        "- Target PMI = 1.23\n",
        "- Error = 1.23 - 0.55 = 0.68\n",
        "- Loss = 0.68\u00b2 = 0.462\n",
        "- Adjusts embeddings to minimize this loss\n",
        "\n",
        "**SWIVEL training (same iteration):**\n",
        "- Also sees the pair (dog, bark) with count = 500\n",
        "- Also targets PMI = 1.23\n",
        "- Also calculates Error = 0.68\n",
        "- Uses weighting function: f(500) = \u221a500 \u2248 22.4 (higher counts get more weight)\n",
        "- Loss = 22.4 \u00d7 0.68\u00b2 \u2248 10.4\n",
        "- Adjusts embeddings more aggressively\n",
        "\n",
        "**Additionally, SWIVEL also processes unobserved pairs:**\n",
        "- Sees the pair (dog, meow) with count = 3\n",
        "- Target PMI = -3.82\n",
        "- Let's say random embeddings give dot product = 0.2\n",
        "- Error = 0.2 - (-3.82) = 4.02\n",
        "- Uses weighting constant: c = 0.75 (small weight for unobserved)\n",
        "- Loss = 0.75 \u00d7 4.02\u00b2 \u2248 12.1\n",
        "- **Pushes the embeddings to make dot product more negative**\n",
        "\n",
        "GloVe never sees this constraint, so it doesn't adjust based on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23: Training approach - Shards vs Direct Matrix Factorization\n",
        "\n",
        "**GloVe's approach:**\n",
        "- Iterates through observed co-occurrences one pair at a time\n",
        "- Training time = Number of observed co-occurrences\n",
        "- Very efficient for sparse matrices\n",
        "\n",
        "**SWIVEL's approach:**\n",
        "- Partitions the co-occurrence matrix into \"shards\" (submatrices)\n",
        "- Each shard contains a block of rows and columns\n",
        "- For each shard, multiplies all the embeddings in that block at once\n",
        "\n",
        "**Example with our 5\u00d75 matrix:**\n",
        "\n",
        "Shard 1 (rows: dog, bark | columns: dog, bark):\n",
        "```\n",
        "       dog  bark\n",
        "dog     0    500\n",
        "bark  500     0\n",
        "```\n",
        "\n",
        "Shard 2 (rows: dog, bark | columns: cat, meow, tree):\n",
        "```\n",
        "       cat  meow  tree\n",
        "dog     45    3    12\n",
        "bark     8    2     1\n",
        "```\n",
        "\n",
        "And so on for all shards.\n",
        "\n",
        "**Why shards?**\n",
        "- Can compute millions of dot products at once using vectorized matrix multiplication\n",
        "- Parallelizable: different shards can be trained on different computers\n",
        "- Makes SWIVEL much faster in practice despite processing more data (including unobserved co-occurrences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24: Why is SWIVEL better on rare words?\n",
        "\n",
        "**Problem with GloVe on rare words:**\n",
        "\n",
        "Imagine a rare word like \"xylophone\" that appears in only 5 sentences total:\n",
        "- It has very few observed co-occurrences\n",
        "- Only 5-10 training signals during GloVe training\n",
        "- Its embedding is under-constrained (not enough information to determine good values)\n",
        "- Result: May end up in a random location in embedding space\n",
        "\n",
        "**Solution in SWIVEL:**\n",
        "\n",
        "SWIVEL considers unobserved co-occurrences:\n",
        "- \"xylophone\" doesn't appear with most words (most pairs are unobserved)\n",
        "- SWIVEL adds constraints: embedding_xylophone should have negative PMI with unrelated words\n",
        "- These constraints regularize where the embedding can be placed\n",
        "- Even with few observed co-occurrences, SWIVEL has many signals from unobserved pairs\n",
        "- Result: Better embedding placement for rare words\n",
        "\n",
        "Empirically, SWIVEL significantly outperforms GloVe on evaluation benchmarks for rare words (like the Rare Words evaluation set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25: Why is SWIVEL faster despite processing more data?\n",
        "\n",
        "**GloVe:**\n",
        "- Must iterate through each observed co-occurrence individually\n",
        "- With a vocabulary of 400,000 words, millions of co-occurrence pairs\n",
        "- Each pair processed sequentially\n",
        "\n",
        "**SWIVEL:**\n",
        "- Processes entire shards using matrix multiplication\n",
        "- Modern GPUs are optimized for matrix multiplication\n",
        "- Can compute thousands of dot products in parallel\n",
        "- Processes both observed and unobserved co-occurrences together in vectorized operations\n",
        "\n",
        "**Trade-off:**\n",
        "- SWIVEL requires computation proportional to the size of the entire matrix (including zeros)\n",
        "- But the vectorized GPU operations more than make up for it\n",
        "- Result: SWIVEL is typically 2-5x faster than GloVe in practice\n",
        "\n",
        "Example timing:\n",
        "- GloVe: 2 hours to train on large corpus\n",
        "- SWIVEL: 20-30 minutes on same corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26: Complete comparison: GloVe vs SWIVEL\n",
        "\n",
        "| Aspect | GloVe | SWIVEL |\n",
        "|--------|-------|--------|\n",
        "| **Co-occurrence handling** | Observed only | Observed + Unobserved |\n",
        "| **Target function** | PMI for observed pairs | Piecewise loss (different weights for observed/unobserved) |\n",
        "| **Training data** | ~2 million pairs for 400k vocab | Entire matrix (~160 billion cells) |\n",
        "| **Processing method** | Sequential pair iteration | Vectorized shard processing |\n",
        "| **Parallelization** | Difficult | Easy (different machines handle different shards) |\n",
        "| **Accuracy (average)** | Baseline | Slightly better overall |\n",
        "| **Accuracy (rare words)** | Poor | Significantly better |\n",
        "| **Training speed** | Slow | Fast (2-5x faster) |\n",
        "| **Final output** | One embedding per word | One embedding per word |\n",
        "| **PMI reconstruction** | Approximates PMI for observed pairs | Approximates PMI for all pairs |\n",
        "\n",
        "**Which to use?**\n",
        "- **GloVe:** Simpler, well-established, good general-purpose embeddings\n",
        "- **SWIVEL:** Better for rare words, faster training, production systems with large datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27: SWIVEL training process step-by-step with dog/bark example\n",
        "\n",
        "**Phase 1: Initialize embeddings** (same as GloVe)\n",
        "- embedding_dog = [0.1, 0.2, 0.3, ..., 0.15]\n",
        "- embedding_bark = [0.1, 0.2, 0.3, ..., 0.20]\n",
        "- And embeddings for cat, meow, tree\n",
        "\n",
        "**Phase 2: Create shards**\n",
        "SWIVEL partitions the matrix into shards. For our small 5\u00d75 matrix, let's say 2 shards:\n",
        "\n",
        "Shard A: rows [dog, bark], columns [dog, bark, cat]\n",
        "Shard B: rows [dog, bark], columns [meow, tree]\n",
        "\n",
        "**Phase 3: Process Shard A**\n",
        "- Compute all embeddings for rows [dog, bark] \u00d7 columns [dog, bark, cat]\n",
        "- For dog\u00d7dog: Target PMI = N/A (diagonal, usually ignored or special handling)\n",
        "- For dog\u00d7bark: Target PMI = 1.23, Current dot product = 0.55, Loss = f(500) \u00d7 (0.55-1.23)\u00b2 = 22.4 \u00d7 0.468 \u2248 10.5\n",
        "- For dog\u00d7cat: Target PMI = log(45/(560\u00d7490)/2000) = log(45/137.2) \u2248 -1.11, Current = 0.3, Loss = f(45) \u00d7 (0.3-(-1.11))\u00b2 = 6.7 \u00d7 1.98 \u2248 13.3\n",
        "- For bark\u00d7dog: Same as dog\u00d7bark\n",
        "- For bark\u00d7bark: Special handling (diagonal)\n",
        "- For bark\u00d7cat: Target PMI = log(8/(511\u00d7490)/2000) \u2248 -1.93, Current = 0.25, Loss = f(8) \u00d7 4.76 \u2248 10.7\n",
        "\n",
        "Total loss for Shard A: 10.5 + 13.3 + 10.7 + ... (more terms)\n",
        "\n",
        "**Phase 4: Backpropagate and update embeddings**\n",
        "- Calculate gradients for all embedding vectors in Shard A\n",
        "- Update: embedding_dog \u2190 embedding_dog - learning_rate \u00d7 gradient_dog\n",
        "- Update: embedding_bark \u2190 embedding_bark - learning_rate \u00d7 gradient_bark\n",
        "- Update: embedding_cat \u2190 embedding_cat - learning_rate \u00d7 gradient_cat\n",
        "\n",
        "**Phase 5: Process Shard B**\n",
        "- Compute embeddings for rows [dog, bark] \u00d7 columns [meow, tree]\n",
        "- For dog\u00d7meow: Target PMI = -3.82, Loss = c \u00d7 (0.2 - (-3.82))\u00b2 = 0.75 \u00d7 16.2 \u2248 12.2 (UNOBSERVED constraint!)\n",
        "- For dog\u00d7tree: Target PMI = log(12/(560\u00d7105)/2000) \u2248 0.38, Loss = f(12) \u00d7 0.01 \u2248 0.04\n",
        "- For bark\u00d7meow, bark\u00d7tree: Similar calculations\n",
        "\n",
        "**The key difference:** SWIVEL processes dog\u00d7meow (count=3) AND if there were a pair never appearing, that unobserved constraint would also be included with weight c.\n",
        "\n",
        "**Phase 6: Repeat**\n",
        "- Cycle through all shards multiple times (epochs)\n",
        "- Due to vectorization, processing all shards is much faster than GloVe's sequential approach\n",
        "- After training completes, embeddings satisfy PMI constraints across the entire matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28: Final embeddings comparison\n",
        "\n",
        "After training, both GloVe and SWIVEL produce embeddings where:\n",
        "- embedding_dog = [0.48, -0.25, 0.70, 0.14, 0.58, ..., -0.34] (300 dimensions)\n",
        "- embedding_bark = [0.55, -0.22, 0.74, 0.10, 0.52, ..., -0.29] (300 dimensions)\n",
        "- dot product(dog, bark) \u2248 1.23\n",
        "\n",
        "**Differences in other embeddings:**\n",
        "\n",
        "For rare word \"xylophone\" (appears only 5 times):\n",
        "\n",
        "**GloVe embedding:**\n",
        "- Trained on just 5 observed co-occurrences\n",
        "- Under-constrained: could be almost anywhere\n",
        "- May be placed randomly\n",
        "\n",
        "**SWIVEL embedding:**\n",
        "- Trained on 5 observed co-occurrences PLUS constraints from hundreds of unobserved pairs\n",
        "- Well-constrained: must avoid being too similar to unrelated words\n",
        "- More likely to end up in a meaningful location\n",
        "- Better performance on rare word similarity tests\n",
        "\n",
        "Both methods produce 300-dimensional embeddings that compress the information, but SWIVEL uses regularization from unobserved co-occurrences to produce better rare word representations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}