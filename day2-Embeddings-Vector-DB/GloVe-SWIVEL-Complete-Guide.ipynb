{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GloVe and SWIVEL Word Embeddings: Complete Q&A Guide\n",
        "\n",
        "*Note: All calculations corrected to account for symmetric co-occurrence matrix (counting unique pairs only)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Co-occurrence Matrices\n",
        "\n",
        "### Q1: What is a co-occurrence matrix?\n",
        "\n",
        "A co-occurrence matrix is a table that tracks which words appear near each other in text. Both rows and columns represent words from your vocabulary, and each cell contains a count of how many times those two words appeared together within a context window (typically 5 words apart).\n",
        "\n",
        "**Example Setup:**\n",
        "Vocabulary: {dog, bark, cat, meow, tree}\n",
        "\n",
        "| | dog | bark | cat | meow | tree |\n",
        "|---|---|---|---|---|---|\n",
        "| dog | 0 | 500 | 45 | 3 | 12 |\n",
        "| bark | 500 | 0 | 8 | 2 | 1 |\n",
        "| cat | 45 | 8 | 0 | 480 | 20 |\n",
        "| meow | 3 | 2 | 480 | 0 | 5 |\n",
        "| tree | 12 | 1 | 20 | 5 | 0 |\n",
        "\n",
        "The value 500 at (dog, bark) means: \"In the corpus, 'dog' and 'bark' appear within a context window 500 times.\"\n",
        "\n",
        "**Important:** This matrix is **symmetric**\u2014(dog, bark) = (bark, dog) = 500. Each unique word pair should be counted only once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Why do we need a co-occurrence matrix?\n",
        "\n",
        "Because of the principle: \"You shall know a word by the company it keeps.\" Words that frequently appear together are likely to be semantically related. The co-occurrence matrix captures these statistical relationships across the entire text corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: The Problem with Large Matrices\n",
        "\n",
        "### Q3: Why is a 400,000 \u00d7 400,000 co-occurrence matrix a problem?\n",
        "\n",
        "A vocabulary of 400,000 words creates a matrix with **160 billion cells**. This is computationally expensive to store and process. Most cells are zeros or very small numbers (sparse data), making this representation wasteful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Matrix Factorization Solution\n",
        "\n",
        "### Q4: How do we solve the size problem?\n",
        "\n",
        "We use **matrix factorization**\u2014specifically Singular Value Decomposition (SVD). The algorithm breaks down the massive 400,000 \u00d7 400,000 matrix into smaller, manageable matrices that can be multiplied back together to approximate the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q5: How exactly does matrix factorization work?\n",
        "\n",
        "The original co-occurrence matrix C is decomposed as:\n",
        "\n",
        "$$C = U \\Sigma V^T$$\n",
        "\n",
        "Where:\n",
        "- U is a 400,000 \u00d7 r matrix\n",
        "- \u03a3 is an r \u00d7 r diagonal matrix of singular values\n",
        "- V^T is an r \u00d7 400,000 matrix\n",
        "\n",
        "The key insight: most information concentrates in just a few singular values. We keep only the **k largest singular values** (e.g., k = 300) and set the rest to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q6: What do we end up with after factorization?\n",
        "\n",
        "After truncation to keep only 300 dimensions:\n",
        "- U becomes 400,000 \u00d7 300\n",
        "- \u03a3 becomes 300 \u00d7 300\n",
        "- V^T becomes 300 \u00d7 400,000\n",
        "\n",
        "We've compressed **160 billion cells down to approximately 240 million cells total**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q7: Why keep 400,000 in the dimension?\n",
        "\n",
        "The 400,000 represents your **vocabulary size**\u2014the number of unique words. Each word needs its own representation. The 300 represents the **embedding dimensions**\u2014the compressed features that capture each word's semantic meaning.\n",
        "\n",
        "**Result:** Each of the 400,000 words gets a 300-dimensional word embedding vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Pointwise Mutual Information (PMI) - The Training Target\n",
        "\n",
        "**IMPORTANT:** Before we can train embeddings, we must first calculate the PMI values from the co-occurrence matrix. These PMI values become the targets that the embeddings are trained to match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q8: What is PMI and why do we need it?\n",
        "\n",
        "PMI (Pointwise Mutual Information) measures whether two words co-occur more or less often than random chance would predict. This is more meaningful than raw co-occurrence counts because it accounts for individual word frequencies.\n",
        "\n",
        "$$\\text{PMI}(\\text{dog, bark}) = \\log\\left(\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})}\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q9: Why not just use P(dog) \u00d7 P(bark)?\n",
        "\n",
        "Great question! Let me explain why we need the full PMI formula instead of just multiplying the marginal probabilities.\n",
        "\n",
        "**The Problem with Just P(dog) \u00d7 P(bark):**\n",
        "\n",
        "If we only used P(dog) \u00d7 P(bark), we'd be calculating the probability that dog and bark co-occur **if they were completely independent** (if their appearance had nothing to do with each other).\n",
        "\n",
        "**Example to show why this matters:**\n",
        "\n",
        "Imagine two scenarios:\n",
        "\n",
        "**Scenario 1: \"dog\" and \"bark\" are semantically related**\n",
        "- They naturally appear together because they're related concepts\n",
        "- Actual co-occurrence: 500 times\n",
        "- If they were random/independent: would only appear together ~250 times (based on P(dog) \u00d7 P(bark))\n",
        "- Result: They appear together **more than chance** predicts\n",
        "\n",
        "**Scenario 2: \"dog\" and \"the\" are not semantically related**\n",
        "- \"the\" is a common word that appears everywhere\n",
        "- Actual co-occurrence: 5,000 times (very high count!)\n",
        "- If they were independent: would appear together ~2,600 times\n",
        "- Result: They appear together roughly as much as chance would predict\n",
        "\n",
        "**The key insight:** Raw co-occurrence counts are misleading. \"dog\" and \"the\" co-occur more frequently (5,000 vs 500), but \"dog\" and \"bark\" are more semantically related. We need PMI to distinguish this difference.\n",
        "\n",
        "PMI fixes this by comparing:\n",
        "- **Actual:** How often they really co-occur\n",
        "- **Expected by chance:** P(dog) \u00d7 P(bark)\n",
        "\n",
        "If actual > expected, PMI is positive (they're attracted to each other semantically).\n",
        "If actual = expected, PMI is zero (they're independent).\n",
        "If actual < expected, PMI is negative (they avoid each other)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Calculate PMI Step-by-Step (CORRECTED)\n",
        "\n",
        "### Q10: How do we calculate PMI step-by-step?\n",
        "\n",
        "**IMPORTANT CORRECTION:** Count only unique pairs (upper triangle of symmetric matrix), not the entire matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1: Calculate total unique co-occurrences**\n",
        "\n",
        "Sum only the unique pairs (upper triangle, excluding diagonal):\n",
        "\n",
        "- dog-bark: 500\n",
        "- dog-cat: 45\n",
        "- dog-meow: 3\n",
        "- dog-tree: 12\n",
        "- bark-cat: 8\n",
        "- bark-meow: 2\n",
        "- bark-tree: 1\n",
        "- cat-meow: 480\n",
        "- cat-tree: 20\n",
        "- meow-tree: 5\n",
        "\n",
        "**Total unique co-occurrences = 500 + 45 + 3 + 12 + 8 + 2 + 1 + 480 + 20 + 5 = 1,076**\n",
        "\n",
        "*(Not 2,000 like before\u2014we're counting each pair only once, not symmetrically)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate the joint probability P(dog, bark)**\n",
        "\n",
        "$$P(\\text{dog, bark}) = \\frac{\\text{co-occurrence count}}{\\text{total unique co-occurrences}} = \\frac{500}{1076} \\approx 0.464$$\n",
        "\n",
        "This is the probability that any randomly selected co-occurrence involves both \"dog\" and \"bark.\"\n",
        "\n",
        "**What this means:** Out of every 1,076 unique word pair co-occurrences we observe, 500 of them are \"dog\" paired with \"bark.\" So there's a 46.4% chance any randomly selected pair is dog-bark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Calculate marginal probability for dog**\n",
        "\n",
        "Dog appears in co-occurrences with: bark (500) + cat (45) + meow (3) + tree (12) = **560 total**\n",
        "\n",
        "$$P(\\text{dog}) = \\frac{560}{1076} \\approx 0.520$$\n",
        "\n",
        "This is the probability that any co-occurrence involves \"dog\" (regardless of what it co-occurs with).\n",
        "\n",
        "**What this means:** Out of every 1,076 unique pairs, 560 of them involve the word \"dog.\" So there's a 52% chance any randomly selected pair includes \"dog.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Calculate marginal probability for bark**\n",
        "\n",
        "Bark appears in co-occurrences with: dog (500) + cat (8) + meow (2) + tree (1) = **511 total**\n",
        "\n",
        "$$P(\\text{bark}) = \\frac{511}{1076} \\approx 0.475$$\n",
        "\n",
        "This is the probability that any co-occurrence involves \"bark\" (regardless of what it co-occurs with).\n",
        "\n",
        "**What this means:** Out of every 1,076 pairs, 511 of them involve the word \"bark.\" So there's a 47.5% chance any randomly selected pair includes \"bark.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Calculate the probability if they were independent**\n",
        "\n",
        "If \"dog\" and \"bark\" appeared together purely by random chance (completely independent), the probability would be:\n",
        "\n",
        "$$P(\\text{dog}) \\times P(\\text{bark}) = 0.520 \\times 0.475 \\approx 0.247$$\n",
        "\n",
        "**What this means:** If dog and bark had nothing to do with each other\u2014if they just randomly happened to appear in the same context windows\u2014they'd co-occur with probability 0.247. Out of 1,076 unique pairs, we'd expect about 265 of them to be \"dog\" with \"bark\" (0.247 \u00d7 1,076 = 265).\n",
        "\n",
        "**This is the crucial comparison:**\n",
        "- **Actual co-occurrence:** 500 times (P(dog, bark) = 0.464)\n",
        "- **Expected by chance:** 265 times (P(dog) \u00d7 P(bark) = 0.247)\n",
        "\n",
        "Dog and bark co-occur 500 \u00f7 265 = **1.88 times MORE OFTEN** than we'd expect if they were independent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Calculate the ratio - Why do we divide?**\n",
        "\n",
        "Now we form the ratio to see how much MORE frequently they co-occur than chance predicts:\n",
        "\n",
        "$$\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})} = \\frac{0.464}{0.247} \\approx 1.88$$\n",
        "\n",
        "**This ratio tells us:**\n",
        "- If ratio > 1: They co-occur MORE often than chance predicts (they're attracted to each other)\n",
        "- If ratio = 1: They co-occur EXACTLY as often as chance would predict (they're independent)\n",
        "- If ratio < 1: They co-occur LESS often than chance predicts (they avoid each other)\n",
        "\n",
        "In our case, the ratio is 1.88, meaning dog and bark co-occur **1.88 times more often than random chance would predict**.\n",
        "\n",
        "**Why divide instead of subtract?**\n",
        "\n",
        "You might ask: \"Why not just calculate P(dog, bark) - P(dog) \u00d7 P(bark) = 0.464 - 0.247 = 0.217?\"\n",
        "\n",
        "Answer: Because subtraction doesn't scale well with different probability ranges.\n",
        "\n",
        "**Example:**\n",
        "- Scenario A: Actual = 0.3, Expected = 0.1 \u2192 Difference = 0.2 \u2192 Ratio = 3.0\n",
        "- Scenario B: Actual = 0.003, Expected = 0.001 \u2192 Difference = 0.002 \u2192 Ratio = 3.0\n",
        "\n",
        "Both scenarios have the same ratio (3.0 times more frequent than expected), but different differences (0.2 vs 0.002). Division preserves the relative relationship regardless of absolute probability values. Subtraction would incorrectly suggest Scenario A is more significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 7: Apply logarithm to get PMI**\n",
        "\n",
        "$$\\text{PMI}(\\text{dog, bark}) = \\log(1.88) \\approx 0.63$$\n",
        "\n",
        "We apply logarithm because:\n",
        "1. It compresses large ratio values (prevents extreme numbers)\n",
        "2. It makes ratios comparable on a symmetric scale (log(3) and log(1/3) have equal magnitude but opposite signs)\n",
        "3. It's mathematically convenient for optimization algorithms\n",
        "\n",
        "**What the 0.63 value means:**\n",
        "- It's the log of how much more often they co-occur than chance predicts\n",
        "- Positive 0.63 means they're semantically related (more related than when PMI = 0)\n",
        "- The magnitude tells us the strength of the relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q11: What does this PMI value tell us?\n",
        "\n",
        "- **PMI > 0:** The words co-occur more often than random chance predicts. They're semantically related and attracted to each other.\n",
        "- **PMI = 0:** They co-occur exactly as random chance would predict. No meaningful relationship.\n",
        "- **PMI < 0:** They co-occur less than random chance predicts. They tend to avoid each other.\n",
        "\n",
        "For our example, PMI \u2248 0.63 is positive, meaning \"dog\" and \"bark\" are semantically related\u2014they co-occur more than chance would predict.\n",
        "\n",
        "**This 0.63 value is now our training target for the embeddings.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Training Word Embeddings to Match PMI\n",
        "\n",
        "### Q12: What is GloVe's training objective?\n",
        "\n",
        "GloVe trains word embeddings so that the dot product between any two word vectors approximates their PMI value:\n",
        "\n",
        "$$\\text{dot product}(\\text{embedding}_{\\text{dog}}, \\text{embedding}_{\\text{bark}}) \\approx \\text{PMI}(\\text{dog, bark}) = 0.63$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q13: How does the training process work?\n",
        "\n",
        "**Step 1: Initialize random embeddings**\n",
        "\n",
        "Start with random 300-dimensional vectors for each word:\n",
        "- embedding_dog = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.15] (300 random numbers)\n",
        "- embedding_bark = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.20] (300 random numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Calculate current dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.1 \\times 0.1) + (0.2 \\times 0.2) + (0.3 \\times 0.3) + \\ldots + (0.15 \\times 0.20)$$\n",
        "\n",
        "Let's say this equals 0.55."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Compare to target PMI**\n",
        "\n",
        "- Target: 0.63\n",
        "- Current: 0.55\n",
        "- Error: 0.63 - 0.55 = 0.08 (too low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Adjust the embeddings**\n",
        "\n",
        "The optimization algorithm (like gradient descent) adjusts the numbers in both embeddings to reduce the error. It might change:\n",
        "- embedding_dog to [0.3, 0.15, 0.5, 0.2, 0.4, ..., 0.25]\n",
        "- embedding_bark to [0.4, 0.2, 0.6, 0.25, 0.5, ..., 0.30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Recalculate dot product**\n",
        "\n",
        "$$\\text{dot product} = (0.3 \\times 0.4) + (0.15 \\times 0.2) + (0.5 \\times 0.6) + \\ldots + (0.25 \\times 0.30)$$\n",
        "\n",
        "Now equals 0.60. Closer to 0.63, so continue adjusting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 6: Repeat iteratively**\n",
        "\n",
        "The algorithm continues adjusting the embedding values thousands of times. After many iterations, it might reach:\n",
        "- embedding_dog = [0.42, -0.18, 0.55, 0.10, 0.45, ..., -0.22]\n",
        "- embedding_bark = [0.48, -0.15, 0.61, 0.08, 0.40, ..., -0.20]\n",
        "\n",
        "$$\\text{dot product} = (0.42 \\times 0.48) + (-0.18 \\times -0.15) + (0.55 \\times 0.61) + \\ldots + (-0.22 \\times -0.20)$$\n",
        "$$= 0.202 + 0.027 + 0.336 + \\ldots + 0.044$$\n",
        "$$= 0.63$$\n",
        "\n",
        "**Success!** The dot product now matches the target PMI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q14: What do these embedding numbers represent?\n",
        "\n",
        "The numbers in the embeddings (like 0.42, -0.18, 0.55, etc.) are **learned parameters discovered through optimization**. They are:\n",
        "\n",
        "- **NOT** co-occurrence counts from the matrix\n",
        "- **NOT** probabilities\n",
        "- **NOT** logarithms of anything\n",
        "- **NOT** mathematically derived from a formula\n",
        "\n",
        "They are simply **values that the algorithm discovered through trial and error** to satisfy the constraint that their dot product equals the target PMI.\n",
        "\n",
        "Think of it like solving a puzzle:\n",
        "- **The puzzle:** Find two sets of 300 numbers that multiply together (dot product) to give 0.63\n",
        "- **The optimizer:** Tries different number combinations repeatedly and measures the error\n",
        "- **The solution:** After many iterations, finds one valid combination\n",
        "\n",
        "There's nothing inherently special about the specific values [0.42, -0.18, 0.55, ...]. A different optimization run might produce different numbers. **As long as the dot product equals 0.63, any set of numbers works.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q15: Does GloVe train embeddings for just one word pair?\n",
        "\n",
        "No! GloVe simultaneously trains embeddings to satisfy the PMI constraint for **all word pairs** in the vocabulary.\n",
        "\n",
        "For our 5-word vocabulary, it needs to satisfy:\n",
        "- dot product(dog, bark) \u2248 0.63\n",
        "- dot product(dog, cat) \u2248 PMI(dog, cat)\n",
        "- dot product(dog, meow) \u2248 PMI(dog, meow)\n",
        "- dot product(dog, tree) \u2248 PMI(dog, tree)\n",
        "- dot product(bark, cat) \u2248 PMI(bark, cat)\n",
        "- ...and all other pairs\n",
        "\n",
        "The algorithm adjusts all embeddings simultaneously to minimize the total error across all word pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Using the Trained Embeddings\n",
        "\n",
        "### Q16: How do we approximate co-occurrence relationships from trained embeddings?\n",
        "\n",
        "After training completes, we can use the **dot product** between two word embedding vectors to approximate their **PMI value**:\n",
        "\n",
        "$$\\text{dot product}(\\text{embedding}_{\\text{dog}}, \\text{embedding}_{\\text{bark}}) \\approx \\text{PMI}(\\text{dog, bark}) = 0.63$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**With the trained embeddings:**\n",
        "- embedding_dog = [0.42, -0.18, 0.55, 0.10, 0.45, ..., -0.22] (300 numbers)\n",
        "- embedding_bark = [0.48, -0.15, 0.61, 0.08, 0.40, ..., -0.20] (300 numbers)\n",
        "\n",
        "**The dot product calculation:**\n",
        "\n",
        "$$\\text{dot product} = (0.42 \\times 0.48) + (-0.18 \\times -0.15) + (0.55 \\times 0.61) + (0.10 \\times 0.08) + (0.45 \\times 0.40) + \\ldots + (-0.22 \\times -0.20)$$\n",
        "\n",
        "$$= 0.202 + 0.027 + 0.336 + 0.008 + 0.180 + \\ldots + 0.044$$\n",
        "\n",
        "$$= 0.63$$\n",
        "\n",
        "This 0.63 tells us that \"dog\" and \"bark\" are semantically related (positive PMI = they co-occur more than chance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q17: Can we get back the original co-occurrence count?\n",
        "\n",
        "Yes, but it requires reversing the PMI calculation:\n",
        "\n",
        "**Step 1: Exponentiate the dot product (PMI)**\n",
        "\n",
        "$$e^{0.63} \\approx 1.88$$\n",
        "\n",
        "This gives us the ratio $\\frac{P(\\text{dog, bark})}{P(\\text{dog}) \\times P(\\text{bark})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Multiply by marginal probabilities**\n",
        "\n",
        "$$1.88 \\times 0.520 \\times 0.475 \\approx 0.464$$\n",
        "\n",
        "This gives us P(dog, bark)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Multiply by total co-occurrences**\n",
        "\n",
        "$$0.464 \\times 1076 \\approx 500$$\n",
        "\n",
        "We've recovered the original co-occurrence count from the co-occurrence matrix!\n",
        "\n",
        "**However, in practice, you rarely need to do this.** For most NLP tasks, you only need the PMI (semantic similarity), which the dot product gives you directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Complete Summary of GloVe\n",
        "\n",
        "### The GloVe Process, Step by Step\n",
        "\n",
        "**Phase 1: Build the co-occurrence matrix**\n",
        "Count how often words appear together in the corpus.\n",
        "- Result: 400,000 \u00d7 400,000 matrix with 160 billion cells\n",
        "- Example: dog and bark co-occur 500 times\n",
        "\n",
        "**Phase 2: Calculate PMI values for all unique word pairs**\n",
        "For each unique word pair, calculate how much more frequently they co-occur than chance would predict.\n",
        "- Calculate total unique pairs = 1,076\n",
        "- Calculate P(dog, bark) = 500 / 1,076 = 0.464\n",
        "- Calculate P(dog) = 560 / 1,076 = 0.520\n",
        "- Calculate P(bark) = 511 / 1,076 = 0.475\n",
        "- Calculate ratio: 0.464 / (0.520 \u00d7 0.475) = 0.464 / 0.247 = 1.88\n",
        "- Apply log: log(1.88) = 0.63\n",
        "- Result: PMI targets for all word pairs\n",
        "- Example: PMI(dog, bark) = 0.63\n",
        "\n",
        "**Phase 3: Initialize random embeddings**\n",
        "Start with random 300-dimensional vectors for each word.\n",
        "- Result: 400,000 \u00d7 300 word embedding matrix with random values\n",
        "\n",
        "**Phase 4: Train embeddings via optimization**\n",
        "Iteratively adjust embedding values so that dot products match PMI targets.\n",
        "- Optimization method: Gradient descent or alternating least squares\n",
        "- Objective: Minimize error between dot products and PMI values across all word pairs\n",
        "- Result: Trained embeddings where dot product(dog, bark) \u2248 0.63\n",
        "\n",
        "**Phase 5: Use embeddings**\n",
        "After training, compute semantic relationships using simple dot products.\n",
        "- Compression: 300 dimensions instead of 400,000\n",
        "- Storage: 240 million cells instead of 160 billion cells\n",
        "- Query: Simple dot product instead of matrix lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why GloVe Works\n",
        "\n",
        "- **Compression:** 300 dimensions capture the essential semantic information that would require 400,000 dimensions in the original matrix.\n",
        "- **Efficiency:** Computing similarity is a simple dot product, not a matrix lookup.\n",
        "- **Semantic meaning:** The dot product reflects meaningful semantic relationships captured by PMI, not just raw frequency.\n",
        "- **Learned representations:** The embedding values are discovered through optimization to satisfy PMI constraints across all word pairs simultaneously.\n",
        "- **Generalization:** Embeddings work for tasks beyond just co-occurrence because they encode deep semantic structure in the learned parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 9: SWIVEL - An Alternative Approach\n",
        "\n",
        "### Q18: What is SWIVEL and how does it differ from GloVe?\n",
        "\n",
        "SWIVEL stands for **Submatrix-wise Vector Embedding Learner**. Like GloVe, it's a count-based method that learns word embeddings from a co-occurrence matrix, but it uses a different approach:\n",
        "\n",
        "**Key differences:**\n",
        "- **GloVe:** Trains only on *observed* co-occurrences (word pairs that appear together)\n",
        "- **SWIVEL:** Trains on *both* observed AND *unobserved* co-occurrences (word pairs that rarely or never appear together)\n",
        "\n",
        "Both methods approximate the PMI matrix, but SWIVEL's special handling of unobserved co-occurrences makes it more accurate on rare words. However, SWIVEL is also considerably faster to train than GloVe in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q19: Why does considering unobserved co-occurrences matter?\n",
        "\n",
        "Let me illustrate with our dog/bark/cat/meow/tree example.\n",
        "\n",
        "**The co-occurrence matrix (all observed pairs):**\n",
        "\n",
        "| | dog | bark | cat | meow | tree |\n",
        "|---|---|---|---|---|---|\n",
        "| dog | 0 | 500 | 45 | 3 | 12 |\n",
        "| bark | 500 | 0 | 8 | 2 | 1 |\n",
        "| cat | 45 | 8 | 0 | 480 | 20 |\n",
        "| meow | 3 | 2 | 480 | 0 | 5 |\n",
        "| tree | 12 | 1 | 20 | 5 | 0 |\n",
        "\n",
        "**What GloVe does:**\n",
        "GloVe only trains on the 10 unique pairs with non-zero values. It completely ignores:\n",
        "- The 5 diagonal cells (dog-dog, bark-bark, etc.)\n",
        "- Any pairs that could be zero or very small due to corpus structure\n",
        "\n",
        "This means GloVe has no constraint on where to place unrelated words' embeddings. If two words never co-occur, GloVe doesn't care if their embeddings point in similar or opposite directions\u2014there's no training signal telling it to push them apart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q20: How does SWIVEL handle unobserved/low co-occurrences?\n",
        "\n",
        "SWIVEL treats pairs with different co-occurrence counts differently:\n",
        "\n",
        "**For high co-occurrences (like dog-bark = 500):**\n",
        "- Target PMI: log(1.88) = 0.63\n",
        "- Train embedding_dog \u00b7 embedding_bark to equal 0.63\n",
        "- Use strong penalty if the dot product differs significantly\n",
        "\n",
        "**For very low co-occurrences (like dog-meow = 3):**\n",
        "- Total: 1,076 pairs\n",
        "- P(dog, meow) = 3 / 1,076 = 0.0028\n",
        "- P(dog) = 0.520, P(meow) = (3 + 2 + 480 + 5) / 1,076 = 490 / 1,076 = 0.455\n",
        "- P(dog) \u00d7 P(meow) = 0.520 \u00d7 0.455 = 0.237\n",
        "- Ratio = 0.0028 / 0.237 = 0.0118 (much less than 1!)\n",
        "- PMI = log(0.0118) \u2248 -4.74\n",
        "\n",
        "**SWIVEL enforces this:** The dot product should be approximately -4.74, not positive. This pushes the dog and meow embeddings in opposite directions, indicating they're unrelated.\n",
        "\n",
        "GloVe ignores this constraint entirely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q21: What is the piecewise loss function in SWIVEL?\n",
        "\n",
        "SWIVEL uses different loss calculations depending on whether a co-occurrence has high or low count:\n",
        "\n",
        "**For high-count co-occurrences (high confidence):**\n",
        "$$\\text{Loss}_{\\text{high}} = f(\\text{count}) \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "Where $$f(\\text{count})$$ is a weighting function that:\n",
        "- Gives higher weight to co-occurrences with higher counts\n",
        "- This makes sense: if two words appear together 500 times, we're very confident they're related\n",
        "- If they appear together only 2 times, we're less certain\n",
        "- Common choice: $f(\\text{count}) = \\sqrt{\\text{count}}$ or $\\min(1, \\text{count}^{0.75})$\n",
        "\n",
        "**For low/zero-count co-occurrences:**\n",
        "$$\\text{Loss}_{\\text{low}} = c \\times (\\text{dot product} - \\text{target PMI})^2$$\n",
        "\n",
        "Where $$c$$ is a small constant (like 0.75).\n",
        "\n",
        "**Why the difference?**\n",
        "- Low-count and zero pairs are very numerous (most word pairs are rare or non-existent)\n",
        "- If we weighted them equally, they'd overwhelm the training signal from high-count pairs\n",
        "- By using a smaller weight $$c$$, SWIVEL balances the learning:\n",
        "  - Learned relationships are primarily driven by high-count co-occurrences\n",
        "  - But low/zero-count pairs provide regularization to prevent rare words from being placed randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q22: Let's compare SWIVEL and GloVe training on dog/bark example\n",
        "\n",
        "**Starting point:** Same random embeddings as GloVe\n",
        "- embedding_dog = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.15] (300 random numbers)\n",
        "- embedding_bark = [0.1, 0.2, 0.3, 0.4, 0.5, ..., 0.20] (300 random numbers)\n",
        "- Current dot product = 0.55\n",
        "\n",
        "**GloVe training:**\n",
        "- Sees the pair (dog, bark) with count = 500\n",
        "- Target PMI = 0.63\n",
        "- Error = 0.63 - 0.55 = 0.08\n",
        "- Loss = 0.08\u00b2 = 0.0064\n",
        "- Adjusts embeddings to minimize this loss\n",
        "\n",
        "**SWIVEL training (same iteration):**\n",
        "- Also sees the pair (dog, bark) with count = 500\n",
        "- Also targets PMI = 0.63\n",
        "- Also calculates Error = 0.08\n",
        "- Uses weighting function: f(500) = \u221a500 \u2248 22.4 (higher counts get more weight)\n",
        "- Loss = 22.4 \u00d7 0.08\u00b2 \u2248 0.143\n",
        "- Adjusts embeddings more aggressively\n",
        "\n",
        "**Additionally, SWIVEL also processes low-count pairs:**\n",
        "- Sees the pair (dog, meow) with count = 3\n",
        "- Target PMI = -4.74\n",
        "- Let's say random embeddings give dot product = 0.2\n",
        "- Error = 0.2 - (-4.74) = 4.94\n",
        "- Uses weighting constant: c = 0.75 (small weight for low-count)\n",
        "- Loss = 0.75 \u00d7 4.94\u00b2 \u2248 18.3\n",
        "- **Pushes the embeddings to make dot product much more negative**\n",
        "\n",
        "GloVe never sees this constraint, so it doesn't adjust based on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q23: Training approach - Shards vs Direct Matrix Factorization\n",
        "\n",
        "**GloVe's approach:**\n",
        "- Iterates through observed co-occurrences one pair at a time\n",
        "- Training time proportional to number of observed co-occurrences (~1,076 pairs in our example)\n",
        "- Very efficient for sparse matrices\n",
        "\n",
        "**SWIVEL's approach:**\n",
        "- Partitions the co-occurrence matrix into \"shards\" (submatrices)\n",
        "- Each shard contains a block of rows and columns\n",
        "- For each shard, processes all embeddings in that block simultaneously using matrix multiplication\n",
        "\n",
        "**Example with our 5\u00d75 matrix:**\n",
        "\n",
        "Shard 1 (rows: dog, bark | columns: dog, bark, cat):\n",
        "```\n",
        "       dog  bark  cat\n",
        "dog     0    500   45\n",
        "bark   500    0    8\n",
        "```\n",
        "\n",
        "Shard 2 (rows: dog, bark | columns: meow, tree):\n",
        "```\n",
        "       meow  tree\n",
        "dog       3    12\n",
        "bark      2     1\n",
        "```\n",
        "\n",
        "And similar shards for other row groups.\n",
        "\n",
        "**Why shards?**\n",
        "- Can compute millions of dot products at once using vectorized matrix multiplication\n",
        "- Parallelizable: different shards can be trained on different computers\n",
        "- Makes SWIVEL much faster in practice despite processing more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q24: Why is SWIVEL better on rare words?\n",
        "\n",
        "**Problem with GloVe on rare words:**\n",
        "\n",
        "Imagine a rare word like \"xylophone\" that appears in only 5 sentences total:\n",
        "- It has very few observed co-occurrences (maybe just 5-10)\n",
        "- Only 5-10 training signals during GloVe training\n",
        "- Its embedding is under-constrained (not enough information to determine good placement)\n",
        "- Result: May end up in a random location in embedding space\n",
        "\n",
        "**Solution in SWIVEL:**\n",
        "\n",
        "SWIVEL considers low/zero co-occurrence constraints:\n",
        "- \"xylophone\" doesn't appear with most words (most pairs have count = 0 or very low)\n",
        "- SWIVEL adds constraints: embedding_xylophone should have negative PMI with unrelated words\n",
        "- These constraints regularize where the embedding can be placed\n",
        "- Even with few observed co-occurrences, SWIVEL has many signals from low-count pairs\n",
        "- Result: Better embedding placement for rare words\n",
        "\n",
        "Empirically, SWIVEL significantly outperforms GloVe on evaluation benchmarks for rare words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q25: Why is SWIVEL faster despite processing more data?\n",
        "\n",
        "**GloVe:**\n",
        "- Must iterate through each observed co-occurrence individually\n",
        "- With a vocabulary of 400,000 words, millions of co-occurrence pairs\n",
        "- Each pair processed sequentially: slow but simple\n",
        "\n",
        "**SWIVEL:**\n",
        "- Processes entire shards using matrix multiplication\n",
        "- Modern GPUs are highly optimized for matrix multiplication (can do thousands in parallel)\n",
        "- Can compute thousands of dot products in a single batch operation\n",
        "- Processes both high-count and low-count pairs together in vectorized operations\n",
        "\n",
        "**Trade-off:**\n",
        "- SWIVEL requires computation proportional to the entire matrix (including low-count cells)\n",
        "- But the vectorized GPU operations more than make up for it\n",
        "- Result: SWIVEL is typically 2-5x faster than GloVe in practice\n",
        "\n",
        "Example timing:\n",
        "- GloVe: 2 hours to train on large corpus\n",
        "- SWIVEL: 20-30 minutes on same corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q26: Complete comparison: GloVe vs SWIVEL\n",
        "\n",
        "| Aspect | GloVe | SWIVEL |\n",
        "|--------|-------|--------|\n",
        "| **Co-occurrence handling** | High-count only | All counts (high + low) |\n",
        "| **Target function** | PMI for high counts | Piecewise loss (different weights) |\n",
        "| **Training data** | ~1 million pairs for 400k vocab | Entire matrix (~160 billion cells) |\n",
        "| **Processing method** | Sequential pair iteration | Vectorized shard processing |\n",
        "| **Parallelization** | Difficult | Easy (different machines handle shards) |\n",
        "| **Accuracy (average)** | Baseline | Slightly better overall |\n",
        "| **Accuracy (rare words)** | Poor | Significantly better |\n",
        "| **Training speed** | Slow | Fast (2-5x faster) |\n",
        "| **Final output** | 300D embedding per word | 300D embedding per word |\n",
        "| **PMI reconstruction** | Approximates PMI for high-count pairs | Approximates PMI for all pairs |\n",
        "\n",
        "**Which to use?**\n",
        "- **GloVe:** Simpler, well-established, good general-purpose embeddings\n",
        "- **SWIVEL:** Better for rare words, faster training, production systems with large datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q27: SWIVEL training process step-by-step with dog/bark example\n",
        "\n",
        "**Phase 1: Initialize embeddings** (same as GloVe)\n",
        "- embedding_dog = [0.1, 0.2, 0.3, ..., 0.15]\n",
        "- embedding_bark = [0.1, 0.2, 0.3, ..., 0.20]\n",
        "- And embeddings for cat, meow, tree\n",
        "\n",
        "**Phase 2: Create shards**\n",
        "SWIVEL partitions the 5\u00d75 matrix into shards. For example:\n",
        "\n",
        "Shard A: rows [dog, bark], columns [dog, bark, cat]\n",
        "Shard B: rows [dog, bark], columns [meow, tree]\n",
        "Shard C: rows [cat, meow], columns [dog, bark, cat]\n",
        "...and more shards\n",
        "\n",
        "**Phase 3: Process Shard A**\n",
        "- Compute all embeddings for rows [dog, bark] \u00d7 columns [dog, bark, cat]\n",
        "- For dog\u00d7dog: count = 0 (diagonal, usually ignored or special handling)\n",
        "- For dog\u00d7bark: count = 500, Target PMI = 0.63, Current dot product = 0.55, Loss = f(500) \u00d7 (0.55-0.63)\u00b2 = 22.4 \u00d7 0.0064 \u2248 0.143\n",
        "- For dog\u00d7cat: count = 45, Calculate PMI = log(45/(1076 \u00d7 0.520 \u00d7 0.48)) \u2248 log(0.18) \u2248 -1.71, Current = 0.3, Loss = f(45) \u00d7 (0.3-(-1.71))\u00b2 = 6.7 \u00d7 4.08 \u2248 27.4\n",
        "- For bark\u00d7dog: Same as dog\u00d7bark (symmetric)\n",
        "- For bark\u00d7bark: Special handling (diagonal)\n",
        "- For bark\u00d7cat: count = 8, Calculate PMI, compute loss similarly\n",
        "\n",
        "Total loss for Shard A: 0.143 + 27.4 + ... (more terms)\n",
        "\n",
        "**Phase 4: Backpropagate and update embeddings**\n",
        "- Calculate gradients for all embedding vectors in Shard A\n",
        "- Update: embedding_dog \u2190 embedding_dog - learning_rate \u00d7 gradient_dog\n",
        "- Update: embedding_bark \u2190 embedding_bark - learning_rate \u00d7 gradient_bark\n",
        "- Update: embedding_cat \u2190 embedding_cat - learning_rate \u00d7 gradient_cat\n",
        "\n",
        "**Phase 5: Process Shard B**\n",
        "- Compute embeddings for rows [dog, bark] \u00d7 columns [meow, tree]\n",
        "- For dog\u00d7meow: count = 3, Target PMI = -4.74, Current = 0.2, Loss = c \u00d7 (0.2 - (-4.74))\u00b2 = 0.75 \u00d7 24.4 \u2248 18.3 (LOW-COUNT constraint!)\n",
        "- For dog\u00d7tree: count = 12, Calculate PMI, compute loss\n",
        "- For bark\u00d7meow, bark\u00d7tree: Similar calculations\n",
        "\n",
        "**The key difference:** SWIVEL processes dog\u00d7meow (count=3) with PMI = -4.74 AND would process any pair with very low count with appropriate negative PMI targets.\n",
        "\n",
        "**Phase 6: Repeat**\n",
        "- Cycle through all shards multiple times (epochs)\n",
        "- Due to vectorization, processing all shards is much faster than GloVe's sequential approach\n",
        "- After training completes, embeddings satisfy PMI constraints across all pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q28: Final embeddings comparison\n",
        "\n",
        "After training, both GloVe and SWIVEL produce embeddings where:\n",
        "- embedding_dog = [0.42, -0.18, 0.55, 0.10, 0.45, ..., -0.22] (300 dimensions)\n",
        "- embedding_bark = [0.48, -0.15, 0.61, 0.08, 0.40, ..., -0.20] (300 dimensions)\n",
        "- dot product(dog, bark) \u2248 0.63\n",
        "\n",
        "**Differences in other embeddings:**\n",
        "\n",
        "For rare word \"xylophone\" (appears only 5 times):\n",
        "\n",
        "**GloVe embedding:**\n",
        "- Trained on just 5 observed co-occurrences\n",
        "- Under-constrained: could be almost anywhere\n",
        "- May be placed randomly\n",
        "- Example: embedding_xylophone = [0.05, 0.08, 0.02, ...] (could be anywhere)\n",
        "\n",
        "**SWIVEL embedding:**\n",
        "- Trained on 5 observed co-occurrences PLUS constraints from hundreds of low-count pairs\n",
        "- Well-constrained: must have negative PMI (around -2 to -3) with most unrelated words\n",
        "- More likely to end up in a meaningful location away from unrelated words\n",
        "- Better performance on rare word similarity tests\n",
        "- Example: embedding_xylophone = [0.35, -0.42, 0.28, ...] (placed in meaningful location)\n",
        "\n",
        "Both methods produce 300-dimensional embeddings that compress the information, but SWIVEL uses regularization from low-count pairs to produce better rare word representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Co-occurrence matrices are symmetric:** Count unique pairs only once, not twice\n",
        "2. **PMI captures semantic relationships:** By comparing actual to expected-by-chance co-occurrence\n",
        "3. **GloVe compresses via learned embeddings:** 300 dimensions instead of 400,000, trained to match PMI targets\n",
        "4. **SWIVEL adds robustness:** By including low-count pairs in training, improving rare word representations\n",
        "5. **Speed vs sophistication:** GloVe is simpler; SWIVEL is faster and more accurate on rare words\n",
        "6. **Embedding values are learned:** Not calculated from formulas, discovered through optimization to satisfy PMI constraints"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}